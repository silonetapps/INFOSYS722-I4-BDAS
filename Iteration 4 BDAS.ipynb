{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-BU\n",
    "INFOSYS722 - Iteration 4 - BDAS\n",
    "Author: S. Schmidt<br>Date: 10/05/2024<br>\n",
    "Desc: Primary question: Is the date the UN set of the year 2030 achievable for Goal 2, Zero world hunger?<br><br>\n",
    "This Spark program reads in the Global Health Index values collected for each country and calculates a Mean value<br>\n",
    "for each collected year. <br>This mean value is then used to predict the date when \"Zero Hunger\" will be reached and in turn, provide insight into when Zero Hunger will be reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 02-DU\n",
    "Load up libraries and retrieve datasource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 17:01:16 WARN Utils: Your hostname, dev2 resolves to a loopback address: 127.0.1.1; using 192.168.1.217 instead (on interface eth0)\n",
      "24/05/11 17:01:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/11 17:01:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.SQLContext import sqlContext\n",
    "spark = SparkSession.builder.appName('predict_un_ghi_target_date').getOrCreate()\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# Import VectorAssembler and Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use Spark to read in the Ecommerce Customers csv file. You can infer csv schemas. \n",
    "data = spark.read.csv(\"global-hunger-index.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Entity: string (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Global Hunger Index (2021): double (nullable = true)\n",
      " |-- 411773-annotations: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema of the DataFrame. You can see potential features as well as the predictor.\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 03-DP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Entity='Afghanistan', Code='AFG', Year=2000, Global Hunger Index (2021)=50.9, 411773-annotations=None),\n",
       " Row(Entity='Afghanistan', Code='AFG', Year=2006, Global Hunger Index (2021)=42.7, 411773-annotations=None),\n",
       " Row(Entity='Afghanistan', Code='AFG', Year=2012, Global Hunger Index (2021)=34.3, 411773-annotations=None),\n",
       " Row(Entity='Afghanistan', Code='AFG', Year=2021, Global Hunger Index (2021)=28.3, 411773-annotations=None),\n",
       " Row(Entity='Albania', Code='ALB', Year=2000, Global Hunger Index (2021)=20.7, 411773-annotations=None),\n",
       " Row(Entity='Albania', Code='ALB', Year=2006, Global Hunger Index (2021)=15.9, 411773-annotations=None),\n",
       " Row(Entity='Albania', Code='ALB', Year=2012, Global Hunger Index (2021)=8.8, 411773-annotations=None),\n",
       " Row(Entity='Albania', Code='ALB', Year=2021, Global Hunger Index (2021)=6.2, 411773-annotations=None),\n",
       " Row(Entity='Algeria', Code='DZA', Year=2000, Global Hunger Index (2021)=14.5, 411773-annotations=None),\n",
       " Row(Entity='Algeria', Code='DZA', Year=2006, Global Hunger Index (2021)=11.7, 411773-annotations=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's focus on one row to make it easier to read.\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Entity='Afghanistan', Code='AFG', Year=2000, Global Hunger Index (2021)=50.9, 411773-annotations=None)\n",
      "Row(Entity='Afghanistan', Code='AFG', Year=2006, Global Hunger Index (2021)=42.7, 411773-annotations=None)\n",
      "Row(Entity='Afghanistan', Code='AFG', Year=2012, Global Hunger Index (2021)=34.3, 411773-annotations=None)\n",
      "Row(Entity='Afghanistan', Code='AFG', Year=2021, Global Hunger Index (2021)=28.3, 411773-annotations=None)\n",
      "Row(Entity='Albania', Code='ALB', Year=2000, Global Hunger Index (2021)=20.7, 411773-annotations=None)\n",
      "Row(Entity='Albania', Code='ALB', Year=2006, Global Hunger Index (2021)=15.9, 411773-annotations=None)\n",
      "Row(Entity='Albania', Code='ALB', Year=2012, Global Hunger Index (2021)=8.8, 411773-annotations=None)\n",
      "Row(Entity='Albania', Code='ALB', Year=2021, Global Hunger Index (2021)=6.2, 411773-annotations=None)\n",
      "Row(Entity='Algeria', Code='DZA', Year=2000, Global Hunger Index (2021)=14.5, 411773-annotations=None)\n",
      "Row(Entity='Algeria', Code='DZA', Year=2006, Global Hunger Index (2021)=11.7, 411773-annotations=None)\n"
     ]
    }
   ],
   "source": [
    "# A simple for loop allows us to make it even clearer. \n",
    "for item in data.head(10):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-DT\n",
    "Data Transform - Process data source to produce Mean values for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Year: int, avg(Global Hunger Index (2021)): double]\n",
      "Column<'Year'>\n",
      "Column<'avg(Global Hunger Index (2021))'>\n",
      "+----+-------+\n",
      "|Year|MeanGHI|\n",
      "+----+-------+\n",
      "|2006|21.0435|\n",
      "|2012|17.5086|\n",
      "|2000|24.4393|\n",
      "|2021|16.7906|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data.sort(['Year']).groupBy(\"Year\").mean(\"Global Hunger Index (2021)\").show()\n",
    "df_raw = data.groupBy(\"Year\").mean(\"Global Hunger Index (2021)\")\n",
    "df_raw = df_raw.withColumnRenamed(\"Global Hunger Index (2021)\", \"MeanGHIraw\")\n",
    "print(df_raw)\n",
    "for item in df_raw:\n",
    "    print(item)\n",
    "#df = df_raw.withColumn('MeanGHI', round(df_raw.MeanGHIraw , 4))\n",
    "#spark.registerDataFrameAsTable(df_raw, \"df_Raw_Table\")\n",
    "#df = spark.sql(\"SELECT Year, `Global Hunger Index (2021)` as MeanGHIraw from df_Raw_Table\")\n",
    "\n",
    "df_raw.createOrReplaceTempView(\"df_Raw_Table\")\n",
    "#df = spark.sql(\"SELECT Year, `Global Hunger Index (2021)` as MeanGHIraw from df_Raw_Table\")\n",
    "df = spark.sql(\"select Year, round(`avg(Global Hunger Index (2021))`, 4) as MeanGHI from df_Raw_Table\").show(n=5)\n",
    "df\n",
    "## 10/05/2024 - Work stopped\n",
    "## Value calculations are correct but pkspark uses ADV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05-DMM\n",
    "Data Transform - Process data source to produce Mean values for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input columns are the feature column names, and the output column is what you'd like the new column to be named. \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Year\", \"Global Hunger Index (2021)\"],\n",
    "    outputCol=\"MeanGHI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've created the assembler variable, let's actually transform the data.\n",
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using print schema, you see that the features output column has been added. \n",
    "output.printSchema()\n",
    "\n",
    "# You can see that the features column is a dense vector that combines the various features as expected.\n",
    "output.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select two columns (the feature and predictor).\n",
    "# This is now in the appropriate format to be processed by Spark.\n",
    "final_data = output.select(\"features\",'Yearly Amount Spent')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a randomised 70/30 split. \n",
    "# Remember, you can use other splits depending on how easy/difficult it is to train your model.\n",
    "train_data,test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see our training data.\n",
    "train_data.describe().show()\n",
    "\n",
    "# And our testing data.\n",
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a Linear Regression Model object. Because the feature column is named 'features', we don't have to worry about it. However, as the labelCol isn't the default name, we have to specify it's name (Yearly Amount Spent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='Yearly Amount Spent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data.\n",
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for linear regression.\n",
    "print(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the model against the test data.\n",
    "test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting results! This shows the difference between the predicted value and the test data.\n",
    "test_results.residuals.show()\n",
    "\n",
    "# Let's get some evaluation metrics (as discussed in the previous linear regression notebook).\n",
    "print(\"RSME: {}\".format(test_results.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the R2 value. \n",
    "print(\"R2: {}\".format(test_results.r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at RMSE and R2, we can see that the model is quite accurate. The RMSE shows that, on average, there's only a \\\\$10 discrepancy between the actual and predicted results. Comparing this to the table below, the average amount spent (\\\\$499) and standard deviation (\\\\$79), a \\\\$10 error is surprisingly good. \n",
    "\n",
    "The R2 also shows that the model accounts for 98% of the variance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what if we didn't have the predictor data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't really relevant to your assignment, but useful in a real-world scenario. What if you have all of these features but no predictor data? How do you actually use the model you've created? Check out the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just select the features column (removing the label column).\n",
    "unlabeled_data = test_data.select('features')\n",
    "unlabeled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can transform the unlabeled data.\n",
    "predictions = lrModel.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It worked! Feeding the unlabeled data features into the model results in a prediction, \n",
    "# which is the amount someone with those features is likely to spend in a year.\n",
    "predictions.show()\n",
    "predictions.head(1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
